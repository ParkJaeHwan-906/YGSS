# -*- coding: utf-8 -*-
"""
Main pipeline orchestrator for ETF/Fund prediction and recommendation system.
Integrates all modules to provide a complete workflow.
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional
import json
import requests
from sklearn.preprocessing import RobustScaler
from dotenv import load_dotenv
load_dotenv()

juptyer_base_path = os.getenv("JUPYTER_BASE_PATH", "")
print(juptyer_base_path)
juptyer_data_path = f"{juptyer_base_path}/data"
fastapi_base_path = os.getenv("FASTAPI_BASE_PATH", "")

# ê° ì„œë¹„ìŠ¤ ëª¨ë“ˆì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from data_service import load_market_data, normalize_asset_df, merge_asset_and_market
from feature_engineering import create_sequences_with_ids, prepare_prophet_data
from model_builder import build_advanced_lstm_model, build_conservative_prophet_model, configure_gpu
from model_manager import ModelManager, create_data_config
from model_evaluator import evaluate_lstm_model, evaluate_prophet_model, evaluate_predictions, compare_models, generate_evaluation_report
from predict_service import predict_all, send_predictions
from recommender import recommend_etfs, generate_recommendation_report
from model_trainer import split_sequences_by_id, create_ensemble_predictions
from sync_service import post_predictions

import pandas as pd
from itertools import combinations
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ì€ ê·¸ëŒ€ë¡œ ìˆë‹¤ê³  ê°€ì • (load_market_data, normalize_asset_df, merge_asset_and_market, create_sequences_with_ids, split_sequences_by_id ë“±)

def find_best_regression_columns(merged_data: pd.DataFrame = None):
    """
    merged_dataì˜ ëª¨ë“  market data ì»¬ëŸ¼ ì¡°í•©ì— ëŒ€í•´ íšŒê·€ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ 
    ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì»¬ëŸ¼ ì¡°í•©ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    if merged_data is None:
        raise ValueError("merged_dataëŠ” í•„ìˆ˜ ì¸ìì…ë‹ˆë‹¤.")

    # íšŒê·€ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
    regression_data = merged_data.dropna()
    
    # 'date', 'id', 'return' ì»¬ëŸ¼ì„ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼ì„ í›„ë³´ íŠ¹ì„±ìœ¼ë¡œ ì§€ì •
    all_possible_features = [col for col in regression_data.columns 
                             if col not in ['date', 'id', 'return']]

    print("í›„ë³´ íŠ¹ì„±:", all_possible_features)
    
    # ëª¨ë“  íŠ¹ì„± ì¡°í•© ìƒì„±
    all_feature_combinations = []
    for i in range(1, len(all_possible_features) + 1):
        for combo in combinations(all_possible_features, i):
            all_feature_combinations.append(list(combo))
    
    print(f"ì´ {len(all_feature_combinations)}ê°œì˜ íŠ¹ì„± ì¡°í•©ì„ íƒìƒ‰í•©ë‹ˆë‹¤.")

    results = {}
    
    # ê° íŠ¹ì„± ì¡°í•©ì— ëŒ€í•´ íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    for combo in all_feature_combinations:
        regression_features = combo
        
        # 'id'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
        X = regression_data[regression_features]
        y = regression_data['return']
        
        # 'id' ì»¬ëŸ¼ì„ ì´ìš©í•˜ì—¬ ì‹œê³„ì—´ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©° ë°ì´í„° ë¶„í• 
        unique_ids = regression_data['id'].unique()
        train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)
        
        X_train = X[regression_data['id'].isin(train_ids)]
        X_test = X[regression_data['id'].isin(test_ids)]
        y_train = y[regression_data['id'].isin(train_ids)]
        y_test = y[regression_data['id'].isin(test_ids)]
        
        # í•™ìŠµ ë°ì´í„°ê°€ ë¹„ì–´ìˆì„ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if X_train.empty or X_test.empty:
            continue
            
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        
        results[tuple(regression_features)] = {'R-squared': r2, 'MAE': mae, 'MSE': mse}
        
    # ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
    if not results:
        print("ê²½ê³ : íšŒê·€ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ íŠ¹ì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return ['oil_price', 'cny_krw', 'kospi', 'interest_rate', 'price_index']

    # ìµœì ì˜ íŠ¹ì„± ì¡°í•© ì„ íƒ (R-squared ê¸°ì¤€)
    best_combo = max(results, key=lambda combo: results[combo]['R-squared'])
    
    print("\nìµœì ì˜ íšŒê·€ íŠ¹ì„± ì¡°í•©:", best_combo)
    print("ìµœê³  R-squared:", results[best_combo]['R-squared'])
    
    return list(best_combo)

def main(asset_data: pd.DataFrame = None, **legacy_kwargs):
    # ... (ê¸°ì¡´ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ë¡œì§)
    configure_gpu()
    try:
        market_data = load_market_data(f"{juptyer_data_path}/market_data.pkl")
        asset_data = normalize_asset_df(asset_data)
        
        # ëª¨ë“  market data ì»¬ëŸ¼ì„ í¬í•¨í•˜ì—¬ ë°ì´í„° ë³‘í•©
        merged_data_all_features, _ = merge_asset_and_market(asset_data, market_data, market_data.columns.tolist())
        
        # find_best_regression_columns í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì ì˜ regression_groupì„ ì°¾ìŒ
        regression_group = find_best_regression_columns(merged_data=merged_data_all_features)
        
        print(f"ìµœì¢… ì„ íƒëœ íšŒê·€ ê·¸ë£¹: {regression_group}")
    
        # ì´í›„ LSTM ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ì„ íƒëœ regression_groupìœ¼ë¡œ ë°ì´í„° ë‹¤ì‹œ ë³‘í•©
        merged_data, model_data = merge_asset_and_market(asset_data, market_data, regression_group)
        
        # Step 2: Feature Engineering (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        lstm_cols = regression_group + ['id', 'return']
        data_for_lstm = model_data[lstm_cols].copy()
        X_sequences_unscaled, y_sequences_unscaled, sequence_ids = create_sequences_with_ids(
            data=data_for_lstm, seq_len=12, target_column='return', id_column='id'
        )
        
        X_train_unscaled, X_test_unscaled, y_train, y_test = split_sequences_by_id(
            X_sequences_unscaled, y_sequences_unscaled, sequence_ids, test_ratio=0.2
        )
        
        
        # Step 3: Model Training and Saving
        print("\n3. ëª¨ë¸ í•™ìŠµ ë° ì €ì¥...")
        print("   - ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì‹œì‘...")
        feature_scaler = RobustScaler()
        target_scaler = RobustScaler()
        y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1))
        X_train_reshaped = X_train_unscaled.reshape(-1, X_train_unscaled.shape[-1])
        X_train_scaled_reshaped = feature_scaler.fit_transform(X_train_reshaped)
        X_train = X_train_scaled_reshaped.reshape(X_train_unscaled.shape)
        X_test_reshaped = X_test_unscaled.reshape(-1, X_test_unscaled.shape[-1])
        X_test_scaled_reshaped = feature_scaler.transform(X_test_reshaped)
        X_test = X_test_scaled_reshaped.reshape(X_test_unscaled.shape)
        y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1))
        print("   - ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ!")
        
        training_results['feature_scaler'] = feature_scaler
        training_results['target_scaler'] = target_scaler
        training_results['X_train'] = X_train
        training_results['X_test'] = X_test
        training_results['y_train'] = y_train_scaled
        training_results['y_test'] = y_test_scaled
        
        print("   - LSTM ëª¨ë¸ ë¹Œë“œ ë° í•™ìŠµ...")
        lstm_model = build_advanced_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))
        lstm_model.fit(X_train, y_train_scaled, epochs=50, batch_size=32, validation_split=0.1, verbose=1)
        training_results['lstm_model'] = lstm_model
        
        print("   - Prophet ëª¨ë¸ ë¹Œë“œ ë° í•™ìŠµ...")
        prophet_model = build_conservative_prophet_model()
        prophet_data_train = prophet_data.iloc[:len(y_train)]
        prophet_model.fit(prophet_data_train)
        training_results['prophet_model'] = prophet_model

        # Step 4: Model Evaluation
        print("\n4. ëª¨ë¸ í‰ê°€...")
        lstm_results = None
        prophet_results = None
        ensemble_results = None
        evaluation_report = ""
        best_model_name = 'LSTM'
        try:
            if training_results.get('lstm_model') and training_results.get('prophet_model'):
                lstm_results = evaluate_lstm_model(model=training_results['lstm_model'], X_test=training_results['X_test'], y_test=training_results['y_test'], target_scaler=training_results['target_scaler'], model_name="LSTM")
                prophet_results = evaluate_prophet_model(model=training_results['prophet_model'], test_dates=model_data.iloc[len(model_data) - len(training_results['y_test']) - sequence_length:].iloc[sequence_length:]['date'], y_test=training_results['y_test'].flatten(), model_name="Prophet")
                ensemble_predictions = create_ensemble_predictions(lstm_results['predictions'], prophet_results['predictions'].reshape(-1, 1))
                ensemble_metrics = evaluate_predictions(training_results['y_test'].flatten(), ensemble_predictions.flatten())
                ensemble_results = {'predictions': ensemble_predictions, 'y_true': training_results['y_test'], 'metrics': ensemble_metrics, 'model_name': 'Ensemble'}
                all_results = [lstm_results, prophet_results, ensemble_results]
                comparison_df = compare_models(all_results)
                best_r2_idx = comparison_df['RÂ²'].idxmax()
                best_model_name = comparison_df.loc[best_r2_idx, 'Model']
                evaluation_report = generate_evaluation_report(all_results)
                print("   - ëª¨ë¸ í‰ê°€ ì™„ë£Œ")
        except Exception as e:
            print(f"ëª¨ë¸ í‰ê°€ ì˜¤ë¥˜: {e}")

        print("\n   - ëª¨ë¸ ì €ì¥ ì‹œì‘...")
        data_config = create_data_config(regression_group, sequence_length, model_data)
        models_to_save = {
            'lstm_model': lstm_model, 'prophet_model': prophet_model,
            'feature_scaler': feature_scaler, 'target_scaler': target_scaler
        }
        training_results['data_config'] = data_config
        saved_hash = model_manager.save_models(models_to_save, data_config, best_model_name)
        print(f"   - ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {saved_hash}")
        
    except Exception as e:
        print(f"ëª¨ë¸ í•™ìŠµ/ì €ì¥ ì˜¤ë¥˜: {e}")
        pipeline_status = "failure"
        return {'error': f'Model training or saving failed: {e}'}


        
    # Step 5: Generate Predictions
    print(f"\n5. ì „ì²´ {asset_type} ì˜ˆì¸¡ ìƒì„±...")
    try:
        # ì˜ˆì¸¡ì— í•„ìš”í•œ ëª¨ë“  ê°ì²´ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        required_keys = ['feature_scaler', 'target_scaler', 'lstm_model', 'prophet_model', 'data_config']
        if not all(key in training_results for key in required_keys):
            raise KeyError(f"ì˜ˆì¸¡ì— í•„ìš”í•œ ëª¨ë¸ ë˜ëŠ” ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤: {required_keys}")
            
        predicted_returns_df = predict_all(
            merged_data=merged_data,
            regression_group=training_results['data_config']['regression_group'],
            sequence_length=sequence_length, 
            training_results=training_results,
            lstm_data=lstm_data,
            id_col='id',
            mode=best_model_name,
            prophet_model=training_results.get('prophet_model'), last_date=model_data['date'].max() if 'date' in model_data.columns else None
        )
        if not predicted_returns_df.empty:
            print(f"   - {len(predicted_returns_df)}ê°œ {asset_type} ì˜ˆì¸¡ ì™„ë£Œ")
            # update_response = post_predictions(predicted_returns_df)
            # print("ìˆ˜ìµë¥  ì—…ë°ì´íŠ¸ ìš”ì²­ ê²°ê³¼", update_response)
        else:
            print(f"   - ì˜ˆì¸¡ ê°€ëŠ¥í•œ {asset_type}ê°€ ì—†ìŠµë‹ˆë‹¤")
            raise ValueError(f"ì˜ˆì¸¡ ê²°ê³¼ DataFrameì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    except (KeyError, ValueError) as e:
        print(f"**{asset_type} ì˜ˆì¸¡ ì˜¤ë¥˜:** {e}")
        predicted_returns_df = pd.DataFrame()
        pipeline_status = "failure"
    except Exception as e:
        print(f"**{asset_type} ì˜ˆì¸¡ ì˜¤ë¥˜:** {e}")
        predicted_returns_df = pd.DataFrame()
        pipeline_status = "failure"
        
    # Step 6: FastAPIì— ëª¨ë¸ ì—…ë¡œë“œ
    # if upload_to_fastapi and saved_hash:
    #     try:
    #         print(f"\n6-2. FastAPI ì—…ë¡œë“œ ì‹œì‘...")
            
    #         base_path_root = Path(__file__).parent.parent / "saved_models"
    #         asset_base_path = base_path_root / asset_type.lower()
    #         metadata_path = asset_base_path / "metadata" / f"metadata_{saved_hash}.json"
    
    #         if not metadata_path.exists():
    #             print(f"  - ì˜¤ë¥˜: ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {metadata_path}")
    #             pipeline_status = "failure"
    #         else:
    #             with open(metadata_path, 'r', encoding='utf-8') as f:
    #                 metadata = json.load(f)
                
    #             upload_list = []
    #             upload_list.append(('metadata', metadata_path))
                
    #             if 'files' in metadata:
    #                 for model_type_key, file_info in metadata['files'].items():
    #                     if 'file_name' in file_info and 'relative_path' in file_info:
    #                         file_path = asset_base_path / file_info['relative_path'] / file_info['file_name']
                
    #                         # ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„: model_typeì„ ë” ê°„ê²°í•˜ê²Œ ì •ì˜
    #                         if 'feature_scaler' in model_type_key:
    #                             simple_model_type = 'feature_scaler'
    #                         elif 'target_scaler' in model_type_key:
    #                             simple_model_type = 'target_scaler'
    #                         else:
    #                             simple_model_type = model_type_key.replace('_model', '')
                            
    #                         upload_list.append((simple_model_type, file_path))

    
    #             upload_url = f"{fastapi_url}/models/upload"
                
    #             for model_type, file_path in upload_list:
    #                 if file_path.exists():
    #                     print(f"  - {model_type} íŒŒì¼ ì—…ë¡œë“œ ì‹œë„: {file_path}")
    #                     with open(file_path, 'rb') as f:
    #                         files = {'model_file': (os.path.basename(file_path), f)}
                            
    #                         data = {'model_type': model_type, 'asset_type': asset_type, 'model_hash': saved_hash, 'force_overwrite': force_retrain}
                            
    #                         response = requests.post(upload_url, files=files, data=data, timeout=600)
    #                         if response.status_code == 200:
    #                             print(f"  - {model_type} ì—…ë¡œë“œ ì„±ê³µ: {response.json()}")
    #                         else:
    #                             print(f"  - {model_type} ì—…ë¡œë“œ ì‹¤íŒ¨: {response.status_code}, ì‘ë‹µ: {response.text}")
    #                             pipeline_status = "failure"
    #                             break
    #                 else:
    #                     print(f"  - ê²½ê³ : íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {file_path}")
    #                     pipeline_status = "failure"
    #                     break
    
    #     except requests.exceptions.RequestException as e:
    #         print(f"  - FastAPI ì—…ë¡œë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")
    #         pipeline_status = "failure"
    # elif upload_to_fastapi and not saved_hash:
    #     print("  - ëª¨ë¸ í•´ì‹œê°€ ì—†ì–´ FastAPI ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœ€")
    
    # Step 7: Compile Results
    results = {
        'evaluation_report': evaluation_report,
        'predicted_returns_df': predicted_returns_df,
        'recommendations': recommendations,
        'recommendation_report': recommendation_report,
        'model_hash': saved_hash,
        'asset_type': asset_type,
        'pipeline_status': pipeline_status
    }

    print("\n" + "=" * 60)
    print("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
    print("=" * 60)
    
    if not predicted_returns_df.empty:
        # ê¸°ì¡´ ì¶œë ¥ ë¡œì§...
        print(f"\nìƒìœ„ 5ê°œ ì˜ˆì¸¡ ìˆ˜ìµë¥ :")
        for idx, row in predicted_returns_df.head().iterrows():
            print(f" Â {row['name']}: {row['predicted_return']:.4f}")
    
    return results

if __name__ == "__main__":
    print("ë°ì´í„°ì…‹ ë¡œë”©...")
    etf_data = pd.read_pickle(f"{juptyer_data_path}/etf_data.pkl")
    etf_data = pd.DataFrame(etf_data)
    etf_data.columns = ['date', 'open', 'close', 'return', 'id']
    etf_data['return'] = pd.to_numeric(etf_data['return'], errors='coerce')
    etf_data['date'] = pd.to_datetime(etf_data['date'], errors='coerce')
    
    print("Fund ë°ì´í„° ë¡œë”©...")
    fund_data = pd.read_pickle(f"{juptyer_data_path}/fund_data.pkl")
    fund_data = pd.DataFrame(fund_data)
    fund_data.columns = ['date', 'open', 'close', 'return', 'id']
    fund_data['return'] = pd.to_numeric(fund_data['return'], errors='coerce')
    fund_data['date'] = pd.to_datetime(fund_data['date'], errors='coerce')
    
    print("\n" + "="*60)
    print("ETF íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    print("="*60)
    # etf_results = main(etf_data, ['kospi', 'oil_price', 'interest_rate', 'price_index', 'usd_krw'], asset_type="ETF")
    etf_results = main(etf_data)
    print("\n" + "="*60)
    print("Fund íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    print("="*60)
    # fund_results = main(fund_data, ['kospi', 'oil_price', 'interest_rate', 'cny_krw', 'usd_krw', 'jpy_krw'], asset_type="Fund")
    fund_results = main(fund_data)
    
    print("\nëª¨ë“  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")